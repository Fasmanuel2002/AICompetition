{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39cef9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from schedulefree import RAdamScheduleFree\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c5c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "seed_everything(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "532fdafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = ...\n",
    "SEGMENTATION_DIR = ...\n",
    "MODEL_DIR = ...\n",
    "OUTPUT_DIR = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fc986a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Configuration:\n",
    "    sequence_lenght: int = 75\n",
    "    number_splits: int = 10\n",
    "    number_seeds: int = 5\n",
    "    number_epochs: int = 15\n",
    "    batch_size: int = 32\n",
    "    learning_rate : float = 5e-3\n",
    "    betas: tuple[float, float] = (0.9,0.999)\n",
    "    label_smoothing: float = 0.1\n",
    "    auxiliar_loss_weight: float = 0.5\n",
    "    \n",
    "configuration = Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d0375ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocessor(to see)\n",
    "\"\"\"\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.set_output(transform=\"pandas\")\n",
    "\n",
    "    def _add_linear_acc(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        acc_values = df.select([\"acc_x\", \"acc_y\", \"acc_z\"]).to_numpy()\n",
    "        quat_values = df.select([\"rot_x\", \"rot_y\", \"rot_z\", \"rot_w\"]).to_numpy()\n",
    "        linear_acc_values = np.full_like(acc_values, np.nan)\n",
    "        gravity_world = np.array([0, 0, 9.81])\n",
    "        for i in range(len(df)):\n",
    "            if np.all(np.isnan(quat_values[i])):\n",
    "                continue\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_values = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_acc_values[i, :] = acc_values[i, :] - gravity_values\n",
    "\n",
    "        df_add = pl.DataFrame(linear_acc_values, schema=[\"linear_acc_x\", \"linear_acc_y\", \"linear_acc_z\"]).fill_nan(None)\n",
    "        df = pl.concat([df, df_add], how=\"horizontal\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _add_rotvec_diff(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        ids = df.select(\"sequence_id\").to_series().to_numpy()\n",
    "        quat_values = df.select([\"rot_x\", \"rot_y\", \"rot_z\", \"rot_w\"]).to_numpy()\n",
    "        rotvec_diff_values = np.full((len(df), 3), np.nan)\n",
    "        for i in range(1, len(df)):\n",
    "            if ids[i - 1] != ids[i]:\n",
    "                continue\n",
    "            q1 = quat_values[i - 1]\n",
    "            q2 = quat_values[i]\n",
    "            if np.all(np.isnan(q1)) or np.all(np.isnan(q2)):\n",
    "                continue\n",
    "            rot1 = R.from_quat(q1)\n",
    "            rot2 = R.from_quat(q2)\n",
    "            rotvec_diff_values[i, :] = (rot1.inv() * rot2).as_rotvec()\n",
    "\n",
    "        df_add = pl.DataFrame(rotvec_diff_values, schema=[\"rotvec_diff_x\", \"rotvec_diff_y\", \"rotvec_diff_z\"]).fill_nan(None)\n",
    "        df = pl.concat([df, df_add], how=\"horizontal\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _cancel_z_rotation(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        quat_values = df.select([\"rot_x\", \"rot_y\", \"rot_z\", \"rot_w\"]).to_numpy()\n",
    "        rotate_flags = df.select(\"rotate\").to_series().to_numpy()\n",
    "        for i in range(len(df)):\n",
    "            if np.all(np.isnan(quat_values[i])):\n",
    "                continue\n",
    "            rotation = R.from_rotvec([0, 0, 130 + 180 * rotate_flags[i]], degrees=True) * R.from_quat(quat_values[i])\n",
    "            quat_values[i, :] = rotation.as_quat(canonical=True, scalar_first=True)\n",
    "\n",
    "        df_add = pl.DataFrame(quat_values, schema=[\"rot_w\", \"rot_x\", \"rot_y\", \"rot_z\"]).fill_nan(None)\n",
    "        df = pl.concat([df.drop([\"rot_w\", \"rot_x\", \"rot_y\", \"rot_z\"]), df_add], how=\"horizontal\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _add_global_acc(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        acc_values = df.select([\"acc_x\", \"acc_y\", \"acc_z\"]).to_numpy()\n",
    "        quat_values = df.select([\"rot_x\", \"rot_y\", \"rot_z\", \"rot_w\"]).to_numpy()\n",
    "        global_acc_values = np.full_like(acc_values, np.nan)\n",
    "        for i in range(len(df)):\n",
    "            if np.all(np.isnan(quat_values[i])):\n",
    "                continue\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            global_acc_values[i, :] = rotation.apply(acc_values[i, :])\n",
    "\n",
    "        df_add = pl.DataFrame(global_acc_values, schema=[\"global_acc_x\", \"global_acc_y\", \"global_acc_z\"]).fill_nan(None)\n",
    "        df = pl.concat([df, df_add], how=\"horizontal\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _handle_left_handed(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        df = (\n",
    "            df.with_columns(\n",
    "                *[\n",
    "                    pl.when(pl.col(\"handedness\") == 0).then(-pl.col(col)).otherwise(pl.col(col)).alias(col)\n",
    "                    for col in [\n",
    "                        \"acc_x\",\n",
    "                        \"linear_acc_x\",\n",
    "                        \"global_acc_x\",\n",
    "                        \"rot_y\",\n",
    "                        \"rot_z\",\n",
    "                        \"rotvec_diff_y\",\n",
    "                        \"rotvec_diff_z\",\n",
    "                    ]\n",
    "                ],\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.when(pl.col(\"handedness\") == 0).then(pl.col(\"thm_5\")).otherwise(pl.col(\"thm_3\")).alias(\"thm_3\"),\n",
    "                pl.when(pl.col(\"handedness\") == 0).then(pl.col(\"thm_3\")).otherwise(pl.col(\"thm_5\")).alias(\"thm_5\"),\n",
    "                *[\n",
    "                    pl.when(pl.col(\"handedness\") == 0)\n",
    "                    .then(pl.col(f\"tof_5_v{i}\"))\n",
    "                    .otherwise(pl.col(f\"tof_3_v{i}\"))\n",
    "                    .alias(f\"tof_3_v{i}\")\n",
    "                    for i in range(64)\n",
    "                ],\n",
    "                *[\n",
    "                    pl.when(pl.col(\"handedness\") == 0)\n",
    "                    .then(pl.col(f\"tof_3_v{i}\"))\n",
    "                    .otherwise(pl.col(f\"tof_5_v{i}\"))\n",
    "                    .alias(f\"tof_5_v{i}\")\n",
    "                    for i in range(64)\n",
    "                ],\n",
    "            )\n",
    "            .with_columns(\n",
    "                *[\n",
    "                    pl.when(pl.col(\"handedness\") == 0)\n",
    "                    .then(pl.col(f\"tof_{i}_v{8 * j + 7 - k}\"))\n",
    "                    .otherwise(pl.col(f\"tof_{i}_v{8 * j + k}\"))\n",
    "                    .alias(f\"tof_{i}_v{8 * j + k}\")\n",
    "                    for i in range(1, 6)\n",
    "                    for j in range(8)\n",
    "                    for k in range(8)\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _handle_rotated_device(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        df = (\n",
    "            df.with_columns(\n",
    "                *[\n",
    "                    pl.when(pl.col(\"rotate\") == 1).then(-pl.col(col)).otherwise(pl.col(col)).alias(col)\n",
    "                    for col in [\n",
    "                        \"acc_x\",\n",
    "                        \"acc_y\",\n",
    "                        \"linear_acc_x\",\n",
    "                        \"linear_acc_y\",\n",
    "                        \"global_acc_x\",\n",
    "                        \"global_acc_y\",\n",
    "                        \"rot_x\",\n",
    "                        \"rot_y\",\n",
    "                        \"rotvec_diff_x\",\n",
    "                        \"rotvec_diff_y\",\n",
    "                    ]\n",
    "                ],\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.when(pl.col(\"rotate\") == 1).then(pl.col(\"thm_4\")).otherwise(pl.col(\"thm_2\")).alias(\"thm_2\"),\n",
    "                pl.when(pl.col(\"rotate\") == 1).then(pl.col(\"thm_2\")).otherwise(pl.col(\"thm_4\")).alias(\"thm_4\"),\n",
    "                pl.when(pl.col(\"rotate\") == 1).then(pl.col(\"thm_5\")).otherwise(pl.col(\"thm_3\")).alias(\"thm_3\"),\n",
    "                pl.when(pl.col(\"rotate\") == 1).then(pl.col(\"thm_3\")).otherwise(pl.col(\"thm_5\")).alias(\"thm_5\"),\n",
    "                *[\n",
    "                    pl.when(pl.col(\"rotate\") == 1)\n",
    "                    .then(pl.col(f\"tof_4_v{i}\"))\n",
    "                    .otherwise(pl.col(f\"tof_2_v{i}\"))\n",
    "                    .alias(f\"tof_2_v{i}\")\n",
    "                    for i in range(64)\n",
    "                ],\n",
    "                *[\n",
    "                    pl.when(pl.col(\"rotate\") == 1)\n",
    "                    .then(pl.col(f\"tof_2_v{i}\"))\n",
    "                    .otherwise(pl.col(f\"tof_4_v{i}\"))\n",
    "                    .alias(f\"tof_4_v{i}\")\n",
    "                    for i in range(64)\n",
    "                ],\n",
    "                *[\n",
    "                    pl.when(pl.col(\"rotate\") == 1)\n",
    "                    .then(pl.col(f\"tof_5_v{i}\"))\n",
    "                    .otherwise(pl.col(f\"tof_3_v{i}\"))\n",
    "                    .alias(f\"tof_3_v{i}\")\n",
    "                    for i in range(64)\n",
    "                ],\n",
    "                *[\n",
    "                    pl.when(pl.col(\"rotate\") == 1)\n",
    "                    .then(pl.col(f\"tof_3_v{i}\"))\n",
    "                    .otherwise(pl.col(f\"tof_5_v{i}\"))\n",
    "                    .alias(f\"tof_5_v{i}\")\n",
    "                    for i in range(64)\n",
    "                ],\n",
    "            )\n",
    "            .with_columns(\n",
    "                *[\n",
    "                    pl.when(pl.col(\"rotate\") == 1)\n",
    "                    .then(pl.col(f\"tof_{i}_v{63 - j}\"))\n",
    "                    .otherwise(pl.col(f\"tof_{i}_v{j}\"))\n",
    "                    .alias(f\"tof_{i}_v{j}\")\n",
    "                    for i in range(1, 6)\n",
    "                    for j in range(64)\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def preprocess(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        if \"rotate\" not in df.columns:\n",
    "            df = df.with_columns(pl.lit(0).alias(\"rotate\"))\n",
    "\n",
    "        df = self._add_linear_acc(df)\n",
    "        df = self._add_rotvec_diff(df)\n",
    "        df = self._cancel_z_rotation(df)\n",
    "        df = self._add_global_acc(df)\n",
    "        df = self._handle_left_handed(df)\n",
    "        df = self._handle_rotated_device(df)\n",
    "\n",
    "        # other preprocessing\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"^thm_.*$\").clip(lower_bound=20).replace({20: None}),\n",
    "            pl.col(\"^tof_._v.*$\").replace({-1: 255}),\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _get_reversed_sequence_tail(self, df: pl.DataFrame, seq_len: int) -> pl.DataFrame:\n",
    "        df_tail = (\n",
    "            df.sort([\"sequence_id\", \"sequence_counter\"], descending=[False, True])\n",
    "            .group_by(\"sequence_id\", maintain_order=True)\n",
    "            .head(seq_len)\n",
    "            .with_columns(pl.col(\"sequence_counter\").cum_count().over(\"sequence_id\"))\n",
    "        )\n",
    "\n",
    "        return df_tail\n",
    "\n",
    "    def get_feature_array(self, df: pl.DataFrame, list_features: list, seq_len: int, fit: bool = False) -> np.ndarray:\n",
    "        df_tail = self._get_reversed_sequence_tail(df, seq_len)\n",
    "        if fit:\n",
    "            self.scaler.fit(df_tail.select(list_features))\n",
    "        df_tail = df_tail.to_pandas()\n",
    "        df_tail[list_features] = self.scaler.transform(df_tail[list_features])\n",
    "        df_tail = pl.from_pandas(df_tail).fill_null(0.0)\n",
    "\n",
    "        feature_arrays = []\n",
    "        for _, df_group in df_tail.group_by(\"sequence_id\", maintain_order=True):\n",
    "            array = df_group.select(list_features).to_numpy().T\n",
    "            array = np.pad(array, ((0, 0), (0, seq_len - array.shape[-1])))\n",
    "            feature_arrays.append(array)\n",
    "\n",
    "        return np.stack(feature_arrays, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29c272bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_gestures = [\n",
    "    \"Above ear - pull hair\",\n",
    "    \"Cheek - pinch skin\",\n",
    "    \"Eyebrow - pull hair\",\n",
    "    \"Eyelash - pull hair\",\n",
    "    \"Forehead - pull hairline\",\n",
    "    \"Forehead - scratch\",\n",
    "    \"Neck - pinch skin\",\n",
    "    \"Neck - scratch\",\n",
    "]\n",
    "\n",
    "non_target_gestures = [\n",
    "    \"Write name on leg\",\n",
    "    \"Wave hello\",\n",
    "    \"Glasses on/off\",\n",
    "    \"Text on phone\",\n",
    "    \"Write name in air\",\n",
    "    \"Feel around in tray and pull out an object\",\n",
    "    \"Scratch knee/leg skin\",\n",
    "    \"Pull air toward your face\",\n",
    "    \"Drink from bottle/cup\",\n",
    "    \"Pinch knee/leg skin\",\n",
    "]\n",
    "\n",
    "all_gestures_in_dataset = target_gestures + non_target_gestures\n",
    "dict_gestures_dataset = {v : i for i, v in enumerate(all_gestures_in_dataset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad977fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\"USING poolars\")\n",
    "df_demographics = pl.read_csv(\"polars\")\n",
    "df = df.join(df_demographics, on=\"subject\", how=\"left\")\n",
    "\n",
    "subjects_rotated = [\"SUBJ_019262\", \"SUBJ_045235\"]\n",
    "df = df.with_columns(pl.col(\"subject\").is_in(subjects_rotated).cast(pl.Int16).alias(\"rotate\"))\n",
    "\n",
    "preprocessing = Preprocessor()\n",
    "df_processed = preprocessing.preprocess(df)\n",
    "\"\"\"\n",
    "Need to implemennt Everything with the dataset\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c828f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Maybe it can Change\n",
    "\"\"\"\n",
    "class TrainDataSet(Dataset):\n",
    "    def __init__(self, X, X_tof, Y = None):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.X_tof = torch.FloatTensor(X_tof)\n",
    "        \n",
    "        if Y is not None:\n",
    "            self.Y = torch.LongTensor(Y)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if \"Y\" not in dir(self):\n",
    "            return (self.X[index], self.X_tof[index], self.Y[index])\n",
    "        return (self.X[index], self.X_tof[index], self.Y[index], torch.Tensor())\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb409ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class Conv2DReLUBN(nn.Module):\n",
    "        def __init__(self, in_channels, output_channels, kernel_size):\n",
    "            super().__init__() \n",
    "            \n",
    "            self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, \n",
    "                      out_channels=output_channels,\n",
    "                      kernel_size=kernel_size, padding=\"same\"),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        def forward(self, x):\n",
    "            return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f95f4781",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToF_2D_Block(nn.Module):\n",
    "    def __init__(self, output_channels, kernel_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            Conv2DReLUBN(\n",
    "               in_channels=1, \n",
    "               output_channels=output_channels, \n",
    "               kernel_size=kernel_size),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            Conv2DReLUBN(\n",
    "                in_channels=output_channels, \n",
    "                output_channels=output_channels, \n",
    "                kernel_size=kernel_size),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            Conv2DReLUBN(\n",
    "                in_channels=output_channels, \n",
    "                output_channels=output_channels,\n",
    "                kernel_size=kernel_size),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54da61c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1DReLUBN_LSTM(nn.Module):\n",
    "    def __init__(self, in_channels, output_channels, kernel_size, stride = 1, groups = 1, lstm_hidden=128):\n",
    "        super().__init__()\n",
    "                        \n",
    "        if stride == 1:\n",
    "            padding = \"same\"\n",
    "        else:\n",
    "            padding = (kernel_size - stride) // 2\n",
    "        \n",
    "        #CNN block\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_channels,\n",
    "                      out_channels=output_channels,\n",
    "                      kernel_size=kernel_size,\n",
    "                      stride=stride,\n",
    "                      padding=padding,\n",
    "                      groups=groups\n",
    "            ),\n",
    "            nn.BatchNorm1d(output_channels),\n",
    "            nn.ReLU())\n",
    "        #LSTM block\n",
    "        self.lstm_block = nn.LSTM(input_size=output_channels, \n",
    "                    hidden_size=lstm_hidden,\n",
    "                    num_layers=1,\n",
    "                    batch_first=True,\n",
    "                    bidirectional=True\n",
    "                    )\n",
    "        self.projection = nn.Linear(2 * lstm_hidden, output_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x : (batch, channels, seq_len)\n",
    "        \"\"\"\n",
    "        #CNN forward\n",
    "        x = self.conv_block(x) \n",
    "        \n",
    "        #Prepare the CNN -> LSTM\n",
    "        x = x.permute(0, 2, 1)#(batch, channels, lenght)\n",
    "        \n",
    "        #LSTM forward\n",
    "        x, _ = self.lstm_block(x)\n",
    "        \n",
    "        #Apply Projection\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        # Return in time-major or batch-major depending on your next block\n",
    "        x = x.permute(0, 2, 1) #(batch, channels, lenght)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "905d0ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi-Layer Perceptron: Linear -> ReLU -> Dropout -> Linear -> ReLU -> Dropout\n",
    "\"\"\"\n",
    "#Fusion the information of different sensor\n",
    "#Concatenate\n",
    "#Introduction no linearity\n",
    "#Prepare the last signal\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, number_channels, mlp_dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(number_channels, number_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(mlp_dropout),\n",
    "            nn.Linear(number_channels // 2, number_channels // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(mlp_dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161e4ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_NET1D(nn.Module):\n",
    "    def __init__(self, in_channles, base_channels=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Encoder, convolutional layers with the Conv1d\n",
    "        self.encoder1 = self.conv_block(in_channels=in_channles, \n",
    "                                        out_channels=base_channels)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.encoder2 = self.conv_block(in_channels=base_channels,\n",
    "                                        out_channels=base_channels * 2)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.encoder3 = self.conv_block(in_channels=base_channels*2,\n",
    "                                        out_channels=base_channels*4)\n",
    "        \n",
    "        #Decoder\n",
    "        self.upcoder2 = nn.ConvTranspose1d(in_channels=base_channels*4,\n",
    "                                           out_channels=base_channels*2,\n",
    "                                           kernel_size=2,\n",
    "                                           stride=2)\n",
    "        self.decoder2 = self.conv_block(in_channels=base_channels*4,\n",
    "                                        out_channels=base_channels*2)\n",
    "        \n",
    "        self.upcoder1 = nn.ConvTranspose1d(in_channels=base_channels*2,\n",
    "                                           out_channels=base_channels,\n",
    "                                           kernel_size=2,\n",
    "                                           stride=2)\n",
    "        self.decoder1 = self.conv_block(in_channels=base_channels*2,\n",
    "                                        out_channels=base_channels)\n",
    "        \n",
    "        #Segmentation Mask\n",
    "        self.out = nn.Conv1d(base_channels, out_channels=1, kernel_size=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_channels,\n",
    "                      out_channels=out_channels, \n",
    "                      kernel_size=3, \n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=out_channels,\n",
    "                      out_channels=out_channels,\n",
    "                      kernel_size=3,\n",
    "                      padding=1\n",
    "            ),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.encoder1(x)\n",
    "        x2 = self.encoder2(self.pool1(x1))\n",
    "        x3 = self.encoder3(self.pool2(x2))\n",
    "        \n",
    "        d2 = self.upcoder2(x3)\n",
    "        d2 = self.decoder2(torch.cat([d2, x2], dim=1))\n",
    "        \n",
    "        d1 = self.upcoder1(d2)\n",
    "        d1 = self.decoder1(torch.cat([d1, x1], dim=1))\n",
    "        \n",
    "        return self.out(d1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4521e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1D_LSTM_Branch(nn.Module):\n",
    "    def __init__(self,\n",
    "                input_channels, \n",
    "                output_channels,\n",
    "                initial_channels_per_feature,\n",
    "                CNN1D_channels_size,\n",
    "                CNN1D_kernel_size,\n",
    "                mlp_dropout,\n",
    "                lstm_hidden = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        \n",
    "        self.inital_layers = Conv1DReLUBN_LSTM(\n",
    "            in_channels=input_channels,\n",
    "            output_channels = input_channels * initial_channels_per_feature,\n",
    "            kernel_size=CNN1D_kernel_size,\n",
    "            stride=1,\n",
    "            groups=input_channels,\n",
    "            lstm_hidden=lstm_hidden\n",
    "        )\n",
    "        CNN1D_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(CNN1D_channels_size)):\n",
    "            if i == 0:\n",
    "                in_channels = input_channels * initial_channels_per_feature\n",
    "            else:\n",
    "                in_channels = CNN1D_channels_size[i - 1]\n",
    "            output_channels = CNN1D_channels_size[i]\n",
    "            \n",
    "            CNN1D_layers.append(\n",
    "                Conv1DReLUBN_LSTM(\n",
    "                    in_channels=in_channels,\n",
    "                    output_channels=output_channels,\n",
    "                    kernel_size=CNN1D_kernel_size,\n",
    "                    stride=1,\n",
    "                    groups=1,\n",
    "                    lstm_hidden=lstm_hidden\n",
    "                )\n",
    "            ) \n",
    "            \n",
    "            if i < len(CNN1D_channels_size) - 1:\n",
    "                CNN1D_layers.append(\n",
    "                    nn.MaxPool1d(kernel_size=3, stride=3)\n",
    "                )\n",
    "        self.CNN1D_layers = nn.Sequential(*CNN1D_layers)\n",
    "        \n",
    "        #original final_channels = CNN1D_channels_size[-1] * 2\n",
    "        final_channels = CNN1D_channels_size[-1] * 2\n",
    "        \n",
    "        self.mlp = MLP(\n",
    "            number_channels=final_channels,\n",
    "            mlp_dropout=mlp_dropout\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, gesture_segment):\n",
    "        x = self.inital_layers(x)\n",
    "        x = self.CNN1D_layers(x)\n",
    "        #Gestures that are part of the target gestures\n",
    "        x1_gestures = (x * (gesture_segment > 0)).sum(dim=2) / (gesture_segment > 0).sum(dim=2).clamp(min=1)\n",
    "        #Non gestures that are part of the non target gestures\n",
    "        x2_non_gestures = (x * (gesture_segment < 0)).sum(dim=2) / (gesture_segment < 0).sum(dim=2).clamp(min=1)\n",
    "        \n",
    "        x = torch.cat([x1_gestures, x2_non_gestures], dim=1)\n",
    "        out = self.mlp(x)\n",
    "        \n",
    "        return x, out\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdc9ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBranchClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        number_imu_blocks,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        initial_channels_per_feature,\n",
    "        cnn1d_channels,\n",
    "        cnn1d_kernel_size,\n",
    "        ToF_out_channels,\n",
    "        ToF_kernel_size,\n",
    "        mlp_dropout,\n",
    "        lstm_hidden=128\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.unet_1d = U_NET1D(in_channles=1)\n",
    "        \n",
    "        self.number_imu_blocks = number_imu_blocks\n",
    "        \n",
    "        self.block_indexes = [1] + [1 + sum(in_channels[: i+1]) for i in range(len(in_channels))]\n",
    "        \n",
    "        self.cnn_branches = nn.ModuleList(\n",
    "             [\n",
    "                CNN1D_LSTM_Branch(\n",
    "                    in_channels[i],\n",
    "                    out_channels,\n",
    "                    initial_channels_per_feature,\n",
    "                    cnn1d_channels,\n",
    "                    cnn1d_kernel_size,\n",
    "                    mlp_dropout,\n",
    "                    lstm_hidden\n",
    "                )\n",
    "                for i in range(len(in_channels))\n",
    "            ] \n",
    "            + [\n",
    "                CNN1D_LSTM_Branch(\n",
    "                    ToF_out_channels * 5,\n",
    "                    out_channels,\n",
    "                    initial_channels_per_feature,\n",
    "                    cnn1d_channels,\n",
    "                    cnn1d_kernel_size,\n",
    "                    mlp_dropout,\n",
    "                    lstm_hidden\n",
    "                )\n",
    "                 \n",
    "                ]\n",
    "        ) \n",
    "        self.tof_block = nn.ModuleList([\n",
    "            ToF_2D_Block(\n",
    "                output_channels=ToF_out_channels,\n",
    "                kernel_size=ToF_kernel_size\n",
    "            )\n",
    "            #5 = for the versions of 5 tofs\n",
    "            for _ in range(5)\n",
    "        ]\n",
    "        )\n",
    "        \n",
    "        n_channels = cnn1d_channels[-1] * (len(in_channels) + 1) * 2\n",
    "        \n",
    "        self.mlp_all = MLP(number_channels=n_channels,mlp_dropout=mlp_dropout)\n",
    "        \n",
    "        self.ensemble_all = nn.Linear((len(in_channels) + 2) * out_channels, out_channels)\n",
    "        \n",
    "        n_channels = cnn1d_channels[-1] * self.number_imu_blocks * 2\n",
    "        \n",
    "        self.mlp_imu = MLP(number_channels=n_channels, mlp_dropout=mlp_dropout)\n",
    "        \n",
    "        self.ensemble_imu = nn.Linear(out_channels * (self.number_imu_blocks + 1), out_features=out_channels)\n",
    "    \n",
    "    def forward(self, x, x_tof):\n",
    "        list_of_x = []\n",
    "        list_out_puts = []\n",
    "        \n",
    "        gesture_segment = torch.sigmoid(self.unet_1d(x[:, :1]))\n",
    "        for i in range(len(self.block_indexes) - 1):\n",
    "            x_block = x[:, self.block_indexes[i] : self.block_indexes[i + 1]]\n",
    "            x_block, out = self.cnn_branches[i](x_block, gesture_segment)\n",
    "            list_of_x.append(x_block)\n",
    "            list_out_puts.append(out)\n",
    "            \n",
    "        list_x_tof = []\n",
    "        for i in range(5):\n",
    "            x_block = x_tof[:,:,i].reshape(-1,1,8,8) #(H:8, W:8)\n",
    "            out = self.tof_block[i](x_block)\n",
    "            out = out.reshape(x.shape[0], -1, out.shape[1]).transpose(1,2)\n",
    "            list_x_tof.append(out)\n",
    "        \n",
    "        x_tof = torch.cat(list_x_tof, dim=1)\n",
    "        x_tof, out = self.cnn_branches[-1](x_tof, gesture_segment)\n",
    "        list_of_x.append(x_tof)\n",
    "        list_out_puts.append(out)\n",
    "        \n",
    "        x_all = torch.cat(list_of_x, dim=1)\n",
    "        out_all = self.mlp_all(x_all)\n",
    "        out_all = self.ensemble_all(torch.cat([out_all] + list_out_puts, dim=1))\n",
    "        \n",
    "        x_imu = torch.cat(list_of_x[: self.number_imu_blocks], dim=1)\n",
    "        out_imu = self.mlp_imu(x_imu)\n",
    "        out_imu = self.ensemble_imu(torch.cat([out_imu] + list_out_puts[: self.number_imu_blocks], dim=1))\n",
    "        \n",
    "        out = torch.stack([out_all, out_imu] + list_out_puts, dim=1)\n",
    "        return out\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e10650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
